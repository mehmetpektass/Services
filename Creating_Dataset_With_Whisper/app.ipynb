{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c884e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "env_path = Path(\".\") / \"env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627c1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence, detect_nonsilent\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28822c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_splitting(audio_path, output_dir, target_duration=20, max_duration=30, min_duration=5, silence_thresh=40):\n",
    "    print(\"Voice File Downloading...\")\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    total_duration = len(audio)/1000\n",
    "\n",
    "    print(f\"Total Time: {total_duration:.2f} seconds\")\n",
    "\n",
    "    target_duration_ms = target_duration * 1000\n",
    "    max_duration_ms = max_duration * 1000\n",
    "    min_duration_ms = min_duration * 1000\n",
    "\n",
    "    nonsilent_ranges = detect_nonsilent(\n",
    "        audio, \n",
    "        min_silence_len=500,  # 0.5 seconds of minimum silence\n",
    "        silence_thresh=silence_thresh\n",
    "    )\n",
    "\n",
    "    if not nonsilent_ranges:\n",
    "        nonsilent_ranges = [(0, len(audio))]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk_start = 0\n",
    "    \n",
    "    for start, end in nonsilent_ranges:\n",
    "        # If the current piece is too long, divide it\n",
    "        while (end - current_chunk_start) > max_duration_ms:\n",
    "            # find the silence closest to the target point\n",
    "            search_start = current_chunk_start + target_duration_ms - 2000  # 2 saniye tolerans\n",
    "            search_end = current_chunk_start + target_duration_ms + 2000\n",
    "            \n",
    "            # Seek silence in this part\n",
    "            best_split_point = find_best_split_point(\n",
    "                audio[search_start:search_end], \n",
    "                search_start, \n",
    "                silence_thresh\n",
    "            )\n",
    "            \n",
    "            if best_split_point:\n",
    "                chunks.append((current_chunk_start, best_split_point))\n",
    "                current_chunk_start = best_split_point\n",
    "            else:\n",
    "                # If silence is not found, use fixed point\n",
    "                chunks.append((current_chunk_start, current_chunk_start + target_duration_ms))\n",
    "                current_chunk_start += target_duration_ms\n",
    "        \n",
    "        # Add the last piece\n",
    "        if (end - current_chunk_start) >= min_duration_ms:\n",
    "            chunks.append((current_chunk_start, end))\n",
    "            current_chunk_start = end\n",
    "\n",
    "    \n",
    "    # Save tracks as a file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    chunk_files = []\n",
    "    \n",
    "    print(f\"{len(chunks)} Sound Chunks Was Created\")\n",
    "\n",
    "    for i, (start, end) in enumerate(chunks):\n",
    "        chunk = audio[start:end]\n",
    "        duration_sec = len(chunk) / 1000\n",
    "        \n",
    "        if duration_sec >= min_duration:\n",
    "            chunk_file = os.path.join(output_dir, f\"chunk_{i:04d}.wav\")\n",
    "            chunk.export(chunk_file, format=\"wav\", parameters=[\"-ar\", \"16000\"])  # 16kHz sampling\n",
    "            \n",
    "            chunk_info = {\n",
    "                'file': chunk_file,\n",
    "                'start_time': start / 1000,\n",
    "                'end_time': end / 1000,\n",
    "                'duration': duration_sec,\n",
    "                'chunk_id': i\n",
    "            }\n",
    "            chunk_files.append(chunk_info)\n",
    "    \n",
    "    print(f\"{len(chunk_files)} Certain Voice Chunks Was Created\")\n",
    "    return chunk_files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split_point(audio_segment, offset, silence_thresh):\n",
    "    # Finds the best dividing point in the given audio segment\n",
    "    # Silence detection\n",
    "    silent_ranges = []\n",
    "    \n",
    "    # Find silence in 100ms windows.\n",
    "    window_size = 100  # ms\n",
    "    for i in range(0, len(audio_segment), window_size):\n",
    "        window = audio_segment[i:i+window_size]\n",
    "        if len(window) > 50 and window.dBFS < silence_thresh:\n",
    "            silent_ranges.append(i + offset)\n",
    "    \n",
    "    # Turn the middle of the longest silence zone\n",
    "    if silent_ranges:\n",
    "        return silent_ranges[len(silent_ranges)//2]\n",
    "    \n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Whisper_Dataset_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
